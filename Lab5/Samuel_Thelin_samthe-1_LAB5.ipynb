{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samuel Thelin - samthe-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def f_ReLU(X):\n",
    "    return np.maximum(0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print >> sys.stderr, err_str\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid): #<-------------------------------------------- activation function, initially sigmoid (might need to change in order to compare with relu)\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_ReLU)) # initially softmax\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist # type: ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helig\\AppData\\Local\\Temp\\ipykernel_964\\383640037.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0]  Training error: 0.90248 Test error: 0.90260\n",
      "[   1]  Training error: 0.88763 Test error: 0.88650\n",
      "[   2]  Training error: 0.90965 Test error: 0.91080\n",
      "[   3]  Training error: 0.90128 Test error: 0.90200\n",
      "[   4]  Training error: 0.90137 Test error: 0.90420\n",
      "[   5]  Training error: 0.90965 Test error: 0.91080\n",
      "[   6]  Training error: 0.89558 Test error: 0.89720\n",
      "[   7]  Training error: 0.90248 Test error: 0.90260\n",
      "[   8]  Training error: 0.90248 Test error: 0.90260\n",
      "[   9]  Training error: 0.90248 Test error: 0.90260\n",
      "[  10]  Training error: 0.90248 Test error: 0.90260\n",
      "[  11]  Training error: 0.89782 Test error: 0.89900\n",
      "[  12]  Training error: 0.88763 Test error: 0.88650\n",
      "[  13]  Training error: 0.90965 Test error: 0.91080\n",
      "[  14]  Training error: 0.90128 Test error: 0.90200\n",
      "[  15]  Training error: 0.90137 Test error: 0.90420\n",
      "[  16]  Training error: 0.89782 Test error: 0.89900\n",
      "[  17]  Training error: 0.90248 Test error: 0.90260\n",
      "[  18]  Training error: 0.90128 Test error: 0.90200\n",
      "[  19]  Training error: 0.89782 Test error: 0.89900\n",
      "[  20]  Training error: 0.89558 Test error: 0.89720\n",
      "[  21]  Training error: 0.90085 Test error: 0.89910\n",
      "[  22]  Training error: 0.89782 Test error: 0.89900\n",
      "[  23]  Training error: 0.90137 Test error: 0.90420\n",
      "[  24]  Training error: 0.90248 Test error: 0.90260\n",
      "[  25]  Training error: 0.90137 Test error: 0.90420\n",
      "[  26]  Training error: 0.90248 Test error: 0.90260\n",
      "[  27]  Training error: 0.88763 Test error: 0.88650\n",
      "[  28]  Training error: 0.90248 Test error: 0.90260\n",
      "[  29]  Training error: 0.90070 Test error: 0.89680\n",
      "[  30]  Training error: 0.89782 Test error: 0.89900\n",
      "[  31]  Training error: 0.90085 Test error: 0.89910\n",
      "[  32]  Training error: 0.90128 Test error: 0.90200\n",
      "[  33]  Training error: 0.90248 Test error: 0.90260\n",
      "[  34]  Training error: 0.90070 Test error: 0.89680\n",
      "[  35]  Training error: 0.90263 Test error: 0.90180\n",
      "[  36]  Training error: 0.90137 Test error: 0.90420\n",
      "[  37]  Training error: 0.88763 Test error: 0.88650\n",
      "[  38]  Training error: 0.90137 Test error: 0.90420\n",
      "[  39]  Training error: 0.90070 Test error: 0.89680\n",
      "[  40]  Training error: 0.90965 Test error: 0.91080\n",
      "[  41]  Training error: 0.90085 Test error: 0.89910\n",
      "[  42]  Training error: 0.90085 Test error: 0.89910\n",
      "[  43]  Training error: 0.90128 Test error: 0.90200\n",
      "[  44]  Training error: 0.90070 Test error: 0.89680\n",
      "[  45]  Training error: 0.90128 Test error: 0.90200\n",
      "[  46]  Training error: 0.90248 Test error: 0.90260\n",
      "[  47]  Training error: 0.90085 Test error: 0.89910\n",
      "[  48]  Training error: 0.88763 Test error: 0.88650\n",
      "[  49]  Training error: 0.88763 Test error: 0.88650\n",
      "[  50]  Training error: 0.90137 Test error: 0.90420\n",
      "[  51]  Training error: 0.90085 Test error: 0.89910\n",
      "[  52]  Training error: 0.88763 Test error: 0.88650\n",
      "[  53]  Training error: 0.90248 Test error: 0.90260\n",
      "[  54]  Training error: 0.90965 Test error: 0.91080\n",
      "[  55]  Training error: 0.88763 Test error: 0.88650\n",
      "[  56]  Training error: 0.90965 Test error: 0.91080\n",
      "[  57]  Training error: 0.88763 Test error: 0.88650\n",
      "[  58]  Training error: 0.90248 Test error: 0.90260\n",
      "[  59]  Training error: 0.90070 Test error: 0.89680\n",
      "[  60]  Training error: 0.90128 Test error: 0.90200\n",
      "[  61]  Training error: 0.90248 Test error: 0.90260\n",
      "[  62]  Training error: 0.90137 Test error: 0.90420\n",
      "[  63]  Training error: 0.90248 Test error: 0.90260\n",
      "[  64]  Training error: 0.90248 Test error: 0.90260\n",
      "[  65]  Training error: 0.88763 Test error: 0.88650\n",
      "[  66]  Training error: 0.90248 Test error: 0.90260\n",
      "[  67]  Training error: 0.90137 Test error: 0.90420\n",
      "[  68]  Training error: 0.90248 Test error: 0.90260\n",
      "[  69]  Training error: 0.90965 Test error: 0.91080\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand the structure of the multilayer perceptron\n",
    "## a. Explain the principle of the backpropagation algorithm\n",
    "As a summary, this algorithm computes the error as the difference between predicted and expected values. This happens in the output layer. As for the hidden layers, the error gets backpropagated layer by layer, using the weights and activation function. This is how our gradients are computed.\n",
    "\n",
    "## b. Explain the meaning and the role of the Softmax function;\n",
    "Softmax is an activation function which is quite different from ones like ReLU or Sigmoid, since it maps *vectors* into a distribution instead of a *value* into a distribution. This is particularly good for things like multi-class classification, since the output is in the form of a 2D-array, as seen in the given code.  \n",
    "\n",
    "## c. Be able to name typically used non-linear output functions and implications of choosing one or another for implementation.\n",
    "Typically used output functions are Softmax and Sigmoid, as they provide a result that can be interpreted as a probability. The implications of using Softmax is that it's great for multi-class classifications, since we input a vector, while Sigmoid does not. Sigmoid, on the other hand, is ideal for tasks that benefit from binary classification, i.e. predicting a 'yes' or a 'no'.\n",
    "\n",
    "## d. The code in the provided Jupyter notebook will stop execution at several points. Find the places in the code, where the execution breaks, answer the questions, comment out the exit line and run the code again.\n",
    "### exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "The line 'self.layers[-1].D = (yhat - labels).T' takes the last layer in the neural network and computes the difference between the predicted output and the actual label. This computes the error of the output layer, which we then store in D, which represents the delta.\n",
    "\n",
    "### #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "As for the for-loop, it iterates through the hidden layers without considering bias (done by slicing W to exclude the last row). Finally, the last computation in the for-loop calculates the delta for the current layer with element-wise multiplication (using dot).\n",
    "\n",
    "### exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "Eta is the learning rate of our model. The updates are done via using the results of the forward-propagated data to then backpropagate them and update the deltas for each layer, creating a gradient. After our gradient is computed, we can update our neurons with new weights (in our case, simply looping through each layer and updating the weights and biases based on the deltas we computed earlier).\n",
    "\n",
    "# 2. Run the code with the suggested config: epochs=70 and eta=0.05. What is the accuracy?\n",
    "The 70th epoch shows: [  69]  Training error: 0.00000 Test error: 0.02670. The accuracy is 1-e, where e is the test error. This gives an accuracy of 97.33%.\n",
    "\n",
    "# 3. Run the code with Learning rate =0.005 and Learning rate =0.5. Explain the observed differences in the functionality of the multi-layer perceptron.\n",
    "Learning rate of 0.005 gave: [  69]  Training error: 0.00047 Test error: 0.02510, while 0.5 gave: [  69]  Training error: 0.90137 Test error: 0.90420. As can be seen, 0.005 is slightly better in comparison to the previous 0.05. 0.5 proves to be significantly worse in comparison to both, with an accuracy of less than 10%.\n",
    "\n",
    "# 4. Extend the code implementing the ReLU output function. Run the perceptron with the suggested by default configuration of hyperparameters: number of epochs = 70 and learning rate = 0.05. What is the classification accuracy? Find the values of the learning rate which results in comparable to Sigmoid case accuracy.\n",
    "By running by the default config values, the result is: [  69]  Training error: 0.90965 Test error: 0.91080, which is less than 9% accuracy. By instead lowering the learning rate to eta=0.005, I got the result: [  69]  Training error: 0.03155 Test error: 0.03720, which is quite similar to the regular configuration which gave a test error of 0.02670."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLAB5",
   "language": "python",
   "name": "envlab5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
